version: "3.7"
services:
  quesma:
    build: ../quesma
    image: quesma:latest
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - TCP_PORT=8080
      - LOGS_PATH=/var/quesma/logs
      - CLICKHOUSE_URL=clickhouse://clickhouse:9000
    depends_on:
      clickhouse:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    links:
      - elasticsearch
    ports:
      - "9999:9999"
      - "8080:8080"
    volumes:
      - ./quesma/logs/:/var/quesma/logs
    restart: unless-stopped
  device-log-generator:
    build: device-log-generator
    image: device-log-generator:latest
    depends_on:
      mitmproxy:
        condition: service_healthy
    links:
      - "mitmproxy"
    restart: unless-stopped
  log-generator:
    build: log-generator
    image: log-generator:latest
    depends_on:
      mitmproxy:
        condition: service_healthy
    links:
      - "mitmproxy"
    restart: unless-stopped
  filebeat:  # Ingests quesma logs to Elasticsearch to be visualized in Kibana
    image: docker.elastic.co/beats/filebeat:8.11.1
    links:
      - "elasticsearch"
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - ./quesma/logs/:/quesma-ingest/
    depends_on:
      quesma:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xmx2G"
    ports:
      - "9201:9200"
      - "9300:9300"
    healthcheck:
      test: curl -s http://elasticsearch:9200 >/dev/null || exit 1
      start_period: 1m
      interval: 1s
      timeout: 1s
    deploy:
      resources:
        limits:
          memory: 4G
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.1
    links:
      - "mitmproxy"
    environment:
      ELASTICSEARCH_HOSTS: '["http://mitmproxy:8080"]'
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTION_KEY: 'QUESMA' # Just to get rid of annoying ERROR in logs
#      LOGGING_ROOT_LEVEL: 'debug'
    depends_on:
      mitmproxy:
        condition: service_healthy
      quesma:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    ports:
      - "5601:5601"
    restart: unless-stopped
    healthcheck:
      test: "curl -s http://localhost:5601/api/status >/dev/null || exit 1"
      start_period: 2m
      interval: 1s
      timeout: 1s
  kibana-sidecar:
    image: docker.elastic.co/kibana/kibana:8.11.1
    restart: "no"
    links:
      - "kibana"
    depends_on:
      kibana:
        condition: service_healthy
    volumes:
      - ./kibana/:/local_mount
    command: ["/bin/bash", "-c", "/local_mount/add_sample_data.sh"]
  clickhouse:
    image: clickhouse/clickhouse-server:23.12.2.59-alpine
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./clickhouse/additional_config.xml:/etc/clickhouse-server/users.d/additional_config.xml
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://clickhouse:8123/ping || exit 1
      interval: 1s
      timeout: 1s
      start_period: 1m
  mitmproxy:
    image: mitmproxy/mitmproxy:10.1.5
    tty: true
    ports:
      - "9200:8080"
      - "8081:8081"
    command: >
      mitmweb --set termlog_verbosity=warn --no-web-open-browser --web-host 0.0.0.0 --mode reverse:http://quesma:8080/
    # -s /var/mitmproxy/request.py
    # Uncomment above, if you would like to log requests using mitmproxy/request.py
    #   The files will be available in mitmproxy/{query,requests}/*.txt
    depends_on:
      quesma:
        condition: service_healthy
    restart: unless-stopped
    volumes:
      - ./mitmproxy:/var/mitmproxy
    healthcheck:
      # No curl/wget, going old school
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/8080' || exit 1
      interval: 1s
      start_period: 1m
      timeout: 1s
  clean:
    build: clean
    depends_on:
      mitmproxy:
        condition: service_started
      clickhouse:
        condition: service_healthy
    restart: "no"
    volumes:
      - ./mitmproxy:/var/mitmproxy
  broker:
    profiles: [kafka]
    image: confluentinc/cp-kafka:7.2.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - '29092:29092'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  kafka-connect:
    profiles: [kafka]
    image: confluentinc/cp-kafka-connect:6.0.0 # bumped from 5.4.9
    container_name: kafka-connect
    ports:
      - '8083:8083'
    depends_on:
      - zookeeper
      - broker
      - elasticsearch
      - quesma
    healthcheck:
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/8083' || exit 1
      interval: 1s
      start_period: 1m
      timeout: 1s
    environment:
      CONNECT_BOOTSTRAP_SERVERS: broker:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: 'connect'
      CONNECT_CONFIG_STORAGE_TOPIC: connect-config
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_REPLICATION_FACTOR: 1
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      # CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'false'
      CONNECT_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
      # CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
      CONNECT_INTERNAL_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: 'io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor'
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: 'io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'connect'
      CONNECT_ZOOKEEPER_CONNECT: zookeeper:2181
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.reflections=ERROR
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-3.3.0.jar
    command:
      - bash
      - -c
      - |
        echo "Installing Connector"
        confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:14.0.12
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
  kafka-connect-sidecar:
    profiles: [kafka]
    image: confluentinc/cp-kafka-connect:6.0.0
    restart: "no"
    links:
      - "kafka-connect"
    depends_on:
      kafka-connect:
        condition: service_healthy
    volumes:
      - ./kafka/:/local_mount
    command: [ "/bin/bash", "-c", "/local_mount/setup-sink.sh" ]
  kafka-producer:
    profiles: [kafka]
    image: confluentinc/cp-kafka-connect:6.0.0
    restart: "no"
    links:
      - "kafka-connect"
    depends_on:
      kafka-connect:
        condition: service_healthy
    volumes:
      - ./kafka/:/local_mount
    command: [ "/bin/bash", "-c", "/local_mount/write-with-console-producer.sh" ]

  zookeeper:
    profiles: [kafka]
    image: confluentinc/cp-zookeeper:7.2.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
